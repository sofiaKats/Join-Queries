# Report Partitioned Hash Join

## Ονόματα - Αριθμοί Μητρώου

Καντάς Σπύρος - 1115201800059

Κατσαούνη Σοφία-Μερόπη - 1115201800070

Μπεληγιάννη Αικατερίνη - 1115201800126

## Έλεγχος Ορθότητας Προγράμματος

Λόγω πολλών σειρών αποτελεσμάτων δεν μπορούμε εύκολα να ξέρουμε αν το πρόγραμμα έτρεξε σωστά οπότε δημιουργήσαμε ένα script που κάνει αυτή τη δουλειά. Το script έχει ένα αρχείο με σωστά αποτελέσματα για αναφορά και τα συγκρίνει με το output του προγράμματος. Τρέχουμε το πρόγραμμα με το flag -t και περνάμε τα αποτελέσματα στο script μέσω pipe:

**./program -t | ../../program_tester/check.sh**

## Το πρόγραμμα

Το πρόγραμμα αρχικά διαβάζει τα αρχεία με τα Relation και τα αποθηκεύει στη δομή Joiner ως μια λίστα από Relation. Έπειτα διαβάζει τα Queries από αρχείο που του δίνεται και εκτελεί τα Join.

## Hopscotch Algorithm

Ο αλγόριθμος hopscotch υλοποιήθηκε σε ένα Hashtable, το οποίο είχε μέγεθος την κοντινότερη δύναμη του 2 στο μέγεθος του αρχικού Relation. Σε κάθε Bucket του hashtable, κρατούνταν το αποθηκευόμενο tuple και το bitmap του Bucket. Στην πρώτη και δεύτερη εργασία, είχε υλοποιηθεί το hashtable, με την χρήση του hopscotch algorithm και την αναδιάταξη των στοιχείων στα πλαίσια μιας γειτονιάς. Το hash function ανάλογα με το μέγεθος του πίνακα χρησιμοποιούσε συγκεκριμένα bits του κάθε αριθμού για να το κατατάξει σε κάποιο bucket. Λόγω όμως του μεγάλου αριθμού των duplicates, καθώς όσο και να αυξανόντουσαν τα bit που θα χρησιμοποιούνταν από το hash function, και αντίστοιχα το μέγεθος του πίνακα, τα duplicates θα γίνονταν hashed στα ίδια μέρη με αποτέλεσμα να έπρεπε να βάλουμε πολύ μεγάλη γειτονιά για να τρέξει το πρόγραμμά μας (πχ 64). Ως αποτέλεσμα αυτό αύξανε πολύ τον χρόνο του προγράμματός μας. Για την τρίτη εργασία λοιπόν, συνδυάσαμε τον αλγόριθμο hopscotch με chaining για τα duplicates, έτσι ώστε να βελτιώσουμε τον χρόνο του προγράμματός μας και να μπορέσουμε να μειώσουμε και το μέγεθος της γειτονιάς. Παρακάτω φαίνεται η διαφορά χρόνου σε σχέση με διάφορες τιμές γειτονιών (στην υλοποίηση με το chaining):

| H | Time | Resizes |
| --- | --- | --- |
| 3 | 8.80647 sec | 3 |
| 4 | 8.42036 sec | 0 |
| 8 | 8.44808 sec | 0 |
| 16 | 8.95723 sec | 0 |
| 32 | 9.67105 sec | 0 |

## Ενδιάμεση Δομή Join (Used Relations)

### 1η υλοποίηση - Πίνακας

Στην υλοποίηση μας χρησιμοποιούμε Πίνακα, ο οποίο έχει τόσες στήλες όσες και τα Relations που χρησιμοποιούνται στο κάθε query. Σε κάθε Join, εξετάζουμε αν το κάθε Relation που είναι να γίνει Join υπάρχει στον πίνακα των UsedRelations, (άρα έχει συμμετάσχει σε προηγούμενο join), παίρνουμε όλα τα στοιχεία του και αφού υλοποιήσουμε το join επιτρέφεται ένας πίνακας απο Matches με τα row ids των στοιχείων που έκαναν match. Μετά, ανανεώνουμε τον πίνακα μας διαγράφοντας τα στοιχεία που δεν υπάρχουν στο Match. Κάτι αρκετά κοστοβόρο στον χρόνο είναι ότι  για κάθε στοιχείο του usedRelations που βρίσκαμε στα matches, έπρεπε να ψάξουμε όλο τον πίνακα των Matches, για να βρούμε όλα τα duplicates. Κάναμε τις παρακάτω προσπάθειες βελτίωσης:

- Μια ιδέα προέκυψε από το γεγονός ότι η PartitionHashJoin δημιουργεί hashtable μόνο για το πρώτο(αριστερό) Relation και με ένα ένα τα στοιχεία του δεύτερου βρίσκει τα κοινά. Αυτό έχει ως αποτέλεσμα ότι τα στοιχεία του δεξιού πίνακα να είναι ταξινομημένα σε πακέτα των ίδιων rowid.
    
    Όταν ενημερώνουμε την usedRelations κοιτάμε αν ένα ένα τα στοιχεία της αν υπάρχουν στα Matches. Αν μόνο η δεξιά(relS) σχέση είναι στη usedRelations και ψάχνουμε για έναν αριθμό πχ 3 τότε μπορούμε όσο βλέπουμε τον αριθμό 3 στα Matches να προσθέτουμε τα στοιχεία και όταν αλλάζει να σταματάμε το ψάξιμο. Αν αντί για την δεξιά(relS) σχέση είναι η αριστερή(relR) σχέση στα usedRelations θα πρέπει αναγκαστικά να ψάξουμε όλα τα Matches. Έτσι σκεφτήκαμε να ορίζουμε τον πίνακα S στην PartitionHashJoin ώς πάντα αυτός που είναι στην usedRelations.
    Αλλά το πρόγραμμα έγινε 2 φορές πιο αργό (14s < 33s), το οποίο πιστεύουμε οφείλοταν στην δημιουργία μεγάλων hashtables γιατί κάποια αρχέια είναι πολύ μεγαλύτερα και πριν ήταν στα δεξιά της ισότητας(1.0 = 2.2) με αποτέλεσμα να μην έχουν hashtables. 
    
- Τέλος,  προέκυψε και μία ακόμα ιδέα, να φτιάχνουμε hashtable για τον μικρότερο πίνακα κάθε φορά. Ο χρόνος ήταν πάλι χειρότερος αλλά υπήρχε βελτίωση σχετικά με την πρώτη ιδέα (14 < 26 < 33).

| R matches | S matches |
| --- | --- |
| 2 | 3 |
| 4 | 3 |
| 12 | 3 |
| 51 | 4 |
| 56 | 4 |
| 56 | 5 |
| 12 | 9 |

### 2η υλοποίηση Λίστα

Μια ιδέα βελτίωσης ήταν η χρήση συνδεδεμένων λιστών ως κύρια δομή της ενδιάμεσης σχέσης usedRelations. 

Ο λόγος ήταν οτι ο πίνακας που χρησιμοποιούμε είναι πολύ μεγαλύτερος απ΄ ότι χρειάζεται, σε κάποιες περιπτώσεις 1 εκ αντί για 14.000 στοιχεία, με αποτέλεσμα να πρέπει να ψάχνουμε όλα τα κελιά για τις μη κενές εγγραφές. Το μέγεθος του ορίζεται από το neighborhood * το μέγεθος του μεγαλύτερου πίνακα. Επομένως κρίνεται απαραίτητη μια δομή που είναι δυναμική.

Για κακή μας τύχη η δομή ήταν πιο αργή κατά 4-6%.
Δεν μπορούμε να καταλάβουμε με σιγουριά που οφείλεται αυτό όμως μάλλον έχει να κάνει με τα cache misses. Σε ένα array που το κάνουμε access sequentially γίνονται πολλά cache hits, επίσης η δέσμευση και αποδέσμευση ενός array είναι αρκετά απλή. Στην περίπτωση της λίστας υπάρχουν πολλοί δείκτες, μια μικρή λίστα θα χωρούσε στην cache όμως στην περίπτωσή μας οι λίστες είναι πολύ μεγάλες (350k) και γίνονται πολλά cache misses. Επίσης κάθε στοιχείο θέλει δέσμευση χώρου και αποδέσμευση.

Η υλοποίηση έχει γίνει στο branch Part3-LL (δεν περιέχει παραλληλοποίηση queries).

## Παραλληλοποίηση

Παραλληλοποίηση έγινε με threadPool που υλοποιεί μία απλή fifo queue όπου μπάινουν τα jobs(συναρτήσεις προς κλήση μαζί με τα arguments τους). Η wait_tasks_to_finish() περιμένει να αδειάσει η queue και λέει στα threads να περιμένουν μέσω bool variable.
Κάθε φορά που ένα thread τελειώνει αυξάνει την μεταβλητή counter και ειδοποιεί τον waiter μέσω cond var να κοιτάξει αν counter = threadsCount.
Εδώ έχει πολλή σημασία στο flow και στον χρόνο πόσο συχνά κάνει lock το mutex ο waiter για να κοιτάξει τον counter etc, για παράδειγμα πριν χρησιμοποιόυσα μια while(1) που έκανε lock όποτε μπορούσε για να δεί το counter με αποτέλεσμα να περιμένουν τα threads ειδικά αν έκανε το ένα lock μετά το άλλο.

Η παραλληλοποίηση στο πρόγραμμα αρχικά έγινε στο partitioning, join. Στη CreateHistogram χωρίζαμε το Relation σε όσα threads υπάρχουν και
το καθένα έφτιαχνε το δικό του histogram που έπειτα αφού τελειώναν όλα τα ενώναμε σε ένα τελικό. Στη BuildPartitionedTable χωρίζαμε το το table και το κάθε thread έβρισκε το index όπου πρέπει να γράψει μέσω PrefixSum έψαχνε κενή θέση έκανε lock το mutex και έγραφε. Αργότερα βρήκαμε έναν καλύτερο τρόπο όπου δεν υπήρχε μόνο ένα mutex αλλά όσα είναι τα PrefixSum buckets επιτρέποντας καλύτερη παραλληλοποίηση. Στην Join χωρίζαμε το RelS σε όσα threads και το καθένα έψαχνε στο hashtable έβρισκε τα αποτελέσματα κλείδωνε το mutex και έγραφε σε έναν τελικό πίκακα Matches. Όλοι αυτοί οι τρόποι συνέβαλαν στην βελτίωση του χρόνου κατά 3-5%.

Λόγω της μικρής βελτίωσης στραφήκαμε στην παραλληλοποίηση των queries η οποία επέφερε τεράστια διαφορά (17.8 < 35, με 2 threads). Το ιδανικό είναι να κρατάγαμε και τις προηγούμενες παραλληλοποιήσεις αλλά η υλοποίηση ήταν αρκετά δύσκολη επομένως οι παραπάνω αλλαγές έχουν μείνει σαν comments στον κώδικα. Παρακάτω φαίνεται η βελτίωση που υπήρξε με βάση των αριθμό στων threads. Όπως φαίνεται ενώ από το 1 στα 2 είχαμε πολύ μεγάλη βελτίωση, όσο αυξάνονταν τα threads, η βελτίωση γινόταν όλο και μικρότερη. Θεωρούμε ότι αυτό έχει να κάνει με το ότι κάποια queries παίρνουν πολύ παραπάνω χρόνο από κάποια άλλα, πχ το 34 (5 sec) με αποτέλεσμα αυτός ο χρόνος να μην επηρεάζεται από αυτή την παραλληλοποίηση.

| #Threads | Time |
| --- | --- |
| 1 | 21.01 sec |
| 2 | 10.843 sec |
| 4 | 8.02 sec |
| 8 | 7.83 sec |

## Join-Enumeration

Για το Join Enumeration χρησιμοποιείται ο αλγόριθμος DP-Linear-1({R1, . . . , Rn}) που αναγράφεται στο paper με κάποιες παραλλαγές. Χρησιμοποιείται η δομή BestTree που είναι ένα array μήκους n, το οποίο κρατάει δείκτες σε κάθε λίστα από subsets των Predicates. Συγκεκριμένα στην πρώτη θέση αποθηκεύεται ένας δείκτης σε λίστα με όλα τα subsets μήκους 1, στην δεύτερη θέση μια λίστα με όλα τα subsets μήκους 2 κλπ. Κάθε λίστα (JoinTreeList) έχει σε κάθε κόμβο ένα JoinTree το οποίο στην υλοποίηση μας αποτελείται από ένα array με Predicates, με την σειρά που είναι να εκτελεστούν, το μήκος του και το κόστος του. To κόστος υπολογίζεται με 2 τρόπους:

- Για κάθε ένα από στα subsets μήκους 1, δηλαδή για κάθε ένα από τα Predicates, ανάλογα το είδος predicate ανανεώνουμε τα φίλτρα και επιστρέφουμε το κόστος για το συγκεκριμένο Predicate.
- Για τα υπόλοιπα subsets μήκους 2 και πάνω, έχει υπολογιστεί ήδη το κόστο για κάποιο από τα Predicates τους, το νέο κόστος υπολογίζεται ως ένα άθροισμα του κόστους των προηγούμενων Predicates + το κόστος του νέου Predicate.

Ο υπολογισμός του κόστους γίνεται ανάλογα σε ποια από τις 5 περιπτώσεις που περιγράφονται στην εργασία και είναι ουσιαστικά το άθροισμα του μήκους όλων των εβδιάμεσων αποτελεσμάτων. Ελέγχεται πάντα να είναι τα predicates μεταξύ τους συνδεδεμένα. Ο αλγόριθμος αυτός δεν επέφερε πολύ μεγάλη διαφορά στον χρόνο αλλά παρατηρούμε ότι ενώ χωρίς αυτόν έγιναν 3 resizes συνολικά, με την χρήση του δεν έγινε κανένα.

## Βελτιώσεις

Οι περισσότερες βελτιώσεις είχαν να κάνουν με την σωστή οργάνωση των δομών, για παράδειγμα σε πολλές δομές με πίνακες προστέθηκε η μεταβλητή activeSize η οποία μας έλεγε πόσα μη στοιχεία έχει ο πίνακας. Αυτό βρήκε εφαρμογή στην usedRelations η οποία έχει τεράστιο size όμως το πραγματικό της size είναι πολύ μικρότερο. Όταν ανανεώνουμε την δομή κοιτάμε ένα ένα τα στοιχεία της και κρατάμε το πόσα είδαμε και αν είναι όσο και το activeSize σταματάμε. Στην Μerge επίσης το prefixSum δεν είχε activeSize και κάθε φορά που θέλαμε να προσθέσουμε στοιχείο το διαβάζαμε μέχρι το τέλος.

Κάτι αντίστοιχο ήταν μια βελτίωση στην BuildPartitionedTable. Εκεί παίρναμε ένα ένα τα στοιχεία του Relation τα κάναμε Ηash και μέσω του PrefixSum βρίσκαμε από που ξεκινάει το bucket του αντίστοιχου hash. Έπειτα βρίσκαμε κενή θέση και βάζαμε την εγγραφή. Αυτό όμως είχε πολλές επαναλήψεις.
Για να το επιλύσουμε φτιάξαμε ένα array με έναν ακέραιο για το κάθε bucket που αντιπροσώπευε το πόσα στοιχεία έχει. Βοήθησε πιο πολύ απ όσο περιμέναμε 7-9%.

Μια άλλη βελτίωση που βοήθησε ήταν στην Merge. Ουσιαστικά το partitioning γίνεται αναδρομικά και κάθε φύλλο γράφει σε μια ενιαία δομή το ταξινομιμένο Relation και PrefixSum. Όταν έχουμε μόνο ένα pass δεν υπάρχει ανάγκη να μεταφέρουμε τα αποτελέσματα γιατί ήδη τα έχουμε σε μια ενιαία δομή. Άρα αυτό που κάνουμε είναι απλά να αλλάζουμε τον pointer να δείχνει σε αυτά.