##Report



###Χρόνοι:

CPU: i7 6500U παλιό φορτωμένο laptop.



###Παραλληλοποίηση:

Παραλληλοποίηση έγινε με threadPool που υλοποιεί μία απλή fifo queue όπου μπάινουν τα jobs(συναρτήσεις προς κλήση μαζί με τα
arguments τους). Η wait_tasks_to_finish() περιμένει να αδειάσει η queue και λέει στα threads να περιμένουν μέσω bool variable.
Κάθε φορά που ένα thread τελειώνει αυξάνει την μεταβλητή counter και ειδοποιεί τον waiter μέσω cond var να κοιτάξει αν counter = threadsCount.
Εδώ έχει πολλή σημασία στο flow και στον χρόνο πόσο συχνά κάνει lock το mutex ο waiter για να κοιτάξει τον counter etc, για παράδειγμα πριν
χρησιμοποιόυσα μια while(1) που έκανε lock όποτε μπορούσε για να δεί το counter με αποτέλεσμα να περιμένουν τα threads ειδικά αν έκανε
το ένα lock μετά το άλλο.

Η παραλληλοποίηση στο πρόγραμμα αρχικά έγινε στο partitioning, join. Στη CreateHistogram χωρίζαμε το Relation σε όσα threads υπάρχουν και
το καθένα έφτιαχνε το δικό του histogram που έπειτα αφού τελειώναν όλα τα ενώναμε σε ένα τελικό. Στη BuildPartitionedTable χωρίζαμε το
το table και το κάθε thread έβρισκε το index όπου πρέπει να γράψει μέσω PrefixSum έψαχνε κενή θέση έκανε lock το mutex και έγραφε. Αργότερα
βρήκαμε έναν καλύτερο τρόπο όπου δεν υπήρχε μόνο ένα mutex αλλά όσα είναι τα PrefixSum buckets επιτρέποντας καλύτερη παραλληλοποίηση. Στην
Join χωρίζαμε το RelS σε όσα threads και το καθένα έψαχνε στο hashtable έβρισκε τα αποτελέσματα κλείδωνε το mutex και έγραφε σε έναν τελικό
πίκακα Matches. Όλοι αυτοί οι τρόποι συνέβαλαν στην βελτίωση του χρόνου κατά 3-5%.

Λόγω της μικρής βελτίωσης στραφήκαμε στην παραλληλοποίηση των queries η οποία επέφερε τεράστια διαφορά (17.8 < 35, 2 threads). Το ιδανικό είναι
να κρατάγαμε και τις προηγούμενες παραλληλοποιήσεις αλλά η υλοποίηση ήταν αρκετά δύσκολη επομένως οι παραπάνω αλλαγές έχουν μείνει σαν comments
στον κώδικα.



###Ιδέες για Βελτιώσεις:

1) Μια ιδέα βελτίωσης ήταν η χρήση συνδεδεμένων λιστών ως κύρια δομή της ενδιάμεσης σχέσης usedRelations.
Ο λόγος ήταν οτι ο πίνακας που χρησιμοποιούμε είναι πολύ μεγαλύτερος απ΄ ότι χρειάζεται, σε κάποιες περιπτώσεις
1 εκ αντί για 14.000 στοιχεία, με αποτέλεσμα να πρέπει να ψάχνουμε όλα τα κελιά για τις μη κενές εγγραφές.
Το μέγεθος του ορίζεται από το neighborhood * το μέγεθος του μεγαλύτερου πίνακα. Επομένως κρίνεται απαραίτητη μια
δομή που είναι δυναμική. Η υλοποίηση έχει γίνει στο branch Part3-LL.

Για κακή μας τύχη η δομή ήταν πιο αργή 1-3 δεύτερα.
Δεν μπορούμε να καταλάβουμε με σιγουριά που οφείλεται αυτό όμως μάλλον έχει να κάνει με τα cache misses. Σε ένα array
που το κάνουμε access sequentially γίνονται πολλά cache hits, επίσης η δέσμευση και αποδέσμευση ενός array είναι αρκετά απλή.
Στην περίπτωση της λίστας υπάρχουν πολλοί δείκτες, μια μικρή λίστα θα χωρούσε στην cache όμως στην περίπτωσή μας οι λίστες
είναι πολύ μεγάλες (350k) και γίνονται πολλά cache misses. Επίσης κάθε στοιχείο θέλει δέσμευση χώρου και αποδέσμευση.

2) Μια άλλη ιδέα προέκυψε από το γεγονός ότι η PartitionHashJoin δημιουργεί hashtable μόνο για το πρώτο(δεξιά) Relation και
με ένα ένα τα στοιχεία του δεύτερου βρίσκει τα κοινά. Αυτό έχει ως αποτέλεσμα ότι τα στοιχεία του αριστερού πίνακα είναι
ταξινομημένα σε πακέτα των ίδιων rowid πχ

-Matches-
R   S
-----
2   3
4   3
12  3
51  4
56  4
56  5
12  9

Όταν ενημερώνουμε την usedRelations κοιτάμε αν ένα ένα τα στοιχεία της υπάρχουν στα Matches. Αν μόνο η S είναι
στη usedRelations και ψάχνουμε για έναν αριθμό πχ 3 τότε μπορούμε όσο βλέπουμε τον αριθμό 3 στα Matches να προσθέτουμε
τα στοιχεία και όταν αλλάζει να σταματάμε το ψάξιμο. Αν αντί για την S είναι η R στα usedRelations θα πρέπει αναγκαστικά
να ψάξουμε όλα τα Matches.

Η ιδέα ήταν να ορίζουμε τον πίνακα S στην PartitionHashJoin ώς πάντα αυτός που έιναι στην usedRelations.
Αλλά το πρόγραμμα έγινε 2 φορές πιο αργό (14s < 33s), πιστεύαμε οφείλοταν στην δημιουργία μεγάλων hashtables γιατί κάποια αρχέια
είναι πολύ μεγαλύτερα και πριν ήταν στα δεξιά της ισότητας(1.0 = 2.2) με αποτέλεσμα να μην έχουν hashtables.

Άρα προέκυψε και μία ακόμα ιδέα, να φτιάχνουμε hashtable για τον μικρότερο πίνακα κάθε φορά. Ο χρόνος ήταν πάλι χειρότερος αλλά
υπήρχε βελτίωση σχετικά με την πρώτη ιδέα (14 < 26 < 33).
