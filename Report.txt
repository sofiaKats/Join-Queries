##Report



###Χρόνοι:

CPU: i7 6500U παλιό φορτωμένο laptop.



###Παραλληλοποίηση:

Παραλληλοποίηση έγινε με threadPool που υλοποιεί μία απλή fifo queue όπου μπάινουν τα jobs(συναρτήσεις προς κλήση μαζί με τα
arguments τους). Η wait_tasks_to_finish() περιμένει να αδειάσει η queue και λέει στα threads να περιμένουν μέσω bool variable.
Κάθε φορά που ένα thread τελειώνει αυξάνει την μεταβλητή counter και ειδοποιεί τον waiter μέσω cond var να κοιτάξει αν counter = threadsCount.
Εδώ έχει πολλή σημασία στο flow και στον χρόνο πόσο συχνά κάνει lock το mutex ο waiter για να κοιτάξει τον counter etc, για παράδειγμα πριν
χρησιμοποιόυσα μια while(1) που έκανε lock όποτε μπορούσε για να δεί το counter με αποτέλεσμα να περιμένουν τα threads ειδικά αν έκανε
το ένα lock μετά το άλλο.

Η παραλληλοποίηση στο πρόγραμμα αρχικά έγινε στο partitioning, join. Στη CreateHistogram χωρίζαμε το Relation σε όσα threads υπάρχουν και
το καθένα έφτιαχνε το δικό του histogram που έπειτα αφού τελειώναν όλα τα ενώναμε σε ένα τελικό. Στη BuildPartitionedTable χωρίζαμε το
το table και το κάθε thread έβρισκε το index όπου πρέπει να γράψει μέσω PrefixSum έψαχνε κενή θέση έκανε lock το mutex και έγραφε. Αργότερα
βρήκαμε έναν καλύτερο τρόπο όπου δεν υπήρχε μόνο ένα mutex αλλά όσα είναι τα PrefixSum buckets επιτρέποντας καλύτερη παραλληλοποίηση. Στην
Join χωρίζαμε το RelS σε όσα threads και το καθένα έψαχνε στο hashtable έβρισκε τα αποτελέσματα κλείδωνε το mutex και έγραφε σε έναν τελικό
πίκακα Matches. Όλοι αυτοί οι τρόποι συνέβαλαν στην βελτίωση του χρόνου κατά 3-5%.

Λόγω της μικρής βελτίωσης στραφήκαμε στην παραλληλοποίηση των queries η οποία επέφερε τεράστια διαφορά (17.8 < 35, με 2 threads). Το ιδανικό είναι
να κρατάγαμε και τις προηγούμενες παραλληλοποιήσεις αλλά η υλοποίηση ήταν αρκετά δύσκολη επομένως οι παραπάνω αλλαγές έχουν μείνει σαν comments
στον κώδικα.


###Βελτιώσεις

Οι περισσότερες βελτιώσεις είχαν να κάνουν με την σωστή οργάνωση των δομών, για παράδειγμα σε πολλές δομές με πίνακες προστέθηκε η μεταβλητή
activeSize η οποία μας έλεγε πόσα μη στοιχεία έχει ο πίνακας. Αυτό βρήκε εφαρμογή στην usedRelations η οποία έχει τεράστιο size όμως το πραγματικό
της size είναι πολύ μικρότερο. Όταν ανανεώνουμε την δομή κοιτάμε ένα ένα τα στοιχεία της κρατάμε το πόσα είδαμε και αν είναι όσο και το activeSize
σταματάμε. Στην Μerge επίσης το prefixSum δεν είχε activeSize και κάθε φορά που θέλαμε να προσθέσουμε στοιχείο το διαβάζαμε μέχρι το τέλος.

Κάτι αντίστοιχο ήταν μια βελτίωση στην BuildPartitionedTable. Εκεί παίρναμε ένα ένα τα στοιχεία του Relation τα κάναμε Ηash και μέσω του PrefixSum
βρίσκαμε από που ξεκινάει το bucket του αντίστοιχου hash. Έπειτα βρίσκαμε κενή θέση και βάζαμε την εγγραφή. Αυτό όμως είχε πολλές επαναλλήψεις.
Για να το επιλύσουμε φτιάξαμε ένα array με έναν ακέραιο για το κάθε bucket που αντιπροσώπευε το πόσα στοιχεία έχει. Βοήθησε πιο πολύ απ όσο περιμέναμε
7-9%.

Μια άλλη βελτίωση που βοήθησε ήταν στην Merge. Ουσιαστικά το partitioning γίνεται αναδρομικά και κάθε φύλλο γράφει σε μια ενιαία δομή το ταξινομιμένο
Relation και PrefixSum. Όταν έχουμε μόνο ένα pass δεν υπάρχει ανάγκη να μεταφέρουμε τα αποτελέσματα γιατί ήδη τα έχουμε σε μια ενιαία δομή. Άρα αυτό
που κάνουμε είναι απλά να αλλάζουμε τον pointer να δείχνει σε αυτά.


###Ιδέες για Βελτιώσεις:

1) Μια ιδέα βελτίωσης ήταν η χρήση συνδεδεμένων λιστών ως κύρια δομή της ενδιάμεσης σχέσης usedRelations.
Ο λόγος ήταν οτι ο πίνακας που χρησιμοποιούμε είναι πολύ μεγαλύτερος απ΄ ότι χρειάζεται, σε κάποιες περιπτώσεις
1 εκ αντί για 14.000 στοιχεία, με αποτέλεσμα να πρέπει να ψάχνουμε όλα τα κελιά για τις μη κενές εγγραφές.
Το μέγεθος του ορίζεται από το neighborhood * το μέγεθος του μεγαλύτερου πίνακα. Επομένως κρίνεται απαραίτητη μια
δομή που είναι δυναμική.

Για κακή μας τύχη η δομή ήταν πιο αργή κατά 4-6%.
Δεν μπορούμε να καταλάβουμε με σιγουριά που οφείλεται αυτό όμως μάλλον έχει να κάνει με τα cache misses. Σε ένα array
που το κάνουμε access sequentially γίνονται πολλά cache hits, επίσης η δέσμευση και αποδέσμευση ενός array είναι αρκετά απλή.
Στην περίπτωση της λίστας υπάρχουν πολλοί δείκτες, μια μικρή λίστα θα χωρούσε στην cache όμως στην περίπτωσή μας οι λίστες
είναι πολύ μεγάλες (350k) και γίνονται πολλά cache misses. Επίσης κάθε στοιχείο θέλει δέσμευση χώρου και αποδέσμευση.

Η υλοποίηση έχει γίνει στο branch Part3-LL (δεν περιέχει παραλληλοποίηση queries).
 
2) Μια άλλη ιδέα προέκυψε από το γεγονός ότι η PartitionHashJoin δημιουργεί hashtable μόνο για το πρώτο(δεξιά) Relation και
με ένα ένα τα στοιχεία του δεύτερου βρίσκει τα κοινά. Αυτό έχει ως αποτέλεσμα ότι τα στοιχεία του αριστερού πίνακα είναι
ταξινομημένα σε πακέτα των ίδιων rowid πχ

--Matches--
R   S
-----
2   3
4   3
12  3
51  4
56  4
56  5
12  9

Όταν ενημερώνουμε την usedRelations κοιτάμε αν ένα ένα τα στοιχεία της υπάρχουν στα Matches. Αν μόνο η S είναι
στη usedRelations και ψάχνουμε για έναν αριθμό πχ 3 τότε μπορούμε όσο βλέπουμε τον αριθμό 3 στα Matches να προσθέτουμε
τα στοιχεία και όταν αλλάζει να σταματάμε το ψάξιμο. Αν αντί για την S είναι η R στα usedRelations θα πρέπει αναγκαστικά
να ψάξουμε όλα τα Matches.

Η ιδέα ήταν να ορίζουμε τον πίνακα S στην PartitionHashJoin ώς πάντα αυτός που έιναι στην usedRelations.
Αλλά το πρόγραμμα έγινε 2 φορές πιο αργό (14s < 33s), πιστεύαμε οφείλοταν στην δημιουργία μεγάλων hashtables γιατί κάποια αρχέια
είναι πολύ μεγαλύτερα και πριν ήταν στα δεξιά της ισότητας(1.0 = 2.2) με αποτέλεσμα να μην έχουν hashtables.

Άρα προέκυψε και μία ακόμα ιδέα, να φτιάχνουμε hashtable για τον μικρότερο πίνακα κάθε φορά. Ο χρόνος ήταν πάλι χειρότερος αλλά
υπήρχε βελτίωση σχετικά με την πρώτη ιδέα (14 < 26 < 33).
